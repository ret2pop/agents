import uuid
import json
import base64
import os
import re
from typing import TypedDict, List, Literal, Optional

# --- RICH UI IMPORTS ---
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from prompt_toolkit import prompt as pt_prompt
from prompt_toolkit.styles import Style as PtStyle

# --- LANGCHAIN / AI IMPORTS ---
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END

# --- IMPORT SPECIALIZED AGENTS AND TOOLS ---
from pyagents.agents.coding_agent import coding_app
from pyagents.agents.deep_research_agent import research_app
from pyagents.agents.math_agent import math_app
from pyagents.tools.rag_tool import LocalLibrarian
from pyagents.tools.search_tool import WebScout
from pyagents.config import ORCHESTRATOR_MODEL, VISION_MODEL, MAX_DEPTH
from pyagents.utils import run_llm, run_vision_llm, clean_text, extract_code

console = Console()

# --- STATE DEFINITION ---
class OrchestratorState(TypedDict):
    user_query: str
    rag_context: str       # Context from local files
    plan: List[str]        # History of steps taken
    current_step: str      # The JSON command for the current turn
    final_answer: str
    depth: int
    max_depth: int

# --- HELPER FUNCTIONS ---

def analyze_image(image_path: str, context_query: str) -> str:
    """Uses a VLM to translate a plot/image into a text description."""

    prompt = (
        f"Context: The user asked to '{context_query}'.\n"
        "This image was generated by the code to answer that request.\n"
        "Describe the plot in detail. What are the key trends, values, or findings?\n"
        "Focus on the DATA shown, not just the aesthetics."
    )

    try:
        console.print(f"[Orchestrator] ðŸ‘ï¸  Analyzing visual output: {image_path}...")
        return run_vision_llm(VISION_MODEL, prompt, image_path, temperature=0.1)
    except Exception as e:
        return f"[Vision Error]: {str(e)}"

# --- TOOL WRAPPERS ---

def call_deep_research(query: str) -> str:
    """Wraps the Deep Research Agent."""
    console.print(f"\n[Orchestrator] ðŸ“š Delegating to Deep Research: '{query}'")

    thread_id = str(uuid.uuid4())[:8]
    config = {"configurable": {"thread_id": thread_id}, "recursion_limit": 150}

    inputs = {
        "main_topic": query,
        "section_plan": [],
        "completed_sections": [],
        "current_section_idx": 0,
        "topic": "init",
        "research_plan": [],
        "research_notes": [],
        "current_draft": "",
        "critiques": [],
        "loop_count": 0
    }

    try:
        result = research_app.invoke(inputs, config=config)
        return f"DEEP RESEARCH REPORT:\n{result['final_report']}"
    except Exception as e:
        return f"DEEP RESEARCH FAILED: {str(e)}"

def call_math_lab(query: str) -> str:
    """Wraps the Math Agent (Triangle of Truth)."""
    console.print(f"\n[Orchestrator] ðŸ§® Delegating to Math Lab: '{query}'")

    inputs = {
        "objective": query,
        "informal_proof": "",
        "lean_code": "",
        "compiler_output": "",
        "success": False,
        "iterations": 0,
        "error_type": None
    }

    result = math_app.invoke(inputs)

    if result["success"]:
        return f"MATH AGENT SUCCESS:\nProof compiled in Lean4.\nCode:\n```lean\n{result['lean_code']}\n```"
    else:
        return f"MATH AGENT FAILED:\nCritique: {result['critique']}\nLast Code Attempt:\n{result['lean_code']}"

def call_code_lab(query: str) -> str:
    """Wraps the Coding Agent (TDD) AND interprets visual output."""
    console.print(f"\n[Orchestrator] ðŸ’» Delegating to Code Lab: '{query}'")

    inputs = {
        "objective": query,
        "code": "",
        "test_code": "",
        "output": "",
        "success": False,
        "iterations": 0,
        "verification_error": None
    }

    result = coding_app.invoke(inputs)

    status = "SUCCESS" if result["success"] else "FAILED"
    text_output = result['output']

    # --- NEW: VISION CHECK ---
    visual_description = ""
    # The Code Agent saves plots to 'output_plot.png' by default
    if result["success"] and os.path.exists("output_plot.png"):
        visual_description = analyze_image("output_plot.png", query)

    final_report = (
        f"CODE AGENT {status}:\n"
        f"--- CONSOLE OUTPUT ---\n{text_output}\n"
    )

    if visual_description:
        final_report += (
            f"\n--- VISUAL OBSERVATION ---\n"
            f"The script generated a plot ('output_plot.png').\n"
            f"Visual Analysis: {visual_description}\n"
        )
    # -------------------------

    return final_report

# Initialize standalone tools
scout = WebScout()

def call_web_scout(query: str) -> str:
    console.print(f"\n[Orchestrator] ðŸŒ Scouting web for: '{query}'")
    return scout.run(query)

# --- NODES ---

def planner_node(state: OrchestratorState):
    """The Brain: Decides which tool to use next."""
    query = state["user_query"]
    context = state["rag_context"]
    depth = state["depth"]

    # If we hit max depth, force a finish
    if depth >= state["max_depth"]:
        return {"current_step": '{"tool": "FINISH", "query": "Max depth reached. Summarizing findings."}'}

    system_prompt = (
        "You are the Research Director. You have 5 specialized departments:\n"
        "1. LIBRARIAN: Checks LOCAL files/codebase. Use for 'refactor', 'existing code', 'project context'.\n"
        "2. WEB_SCOUT: Quick internet search. Use for specific facts, docs, or recent events.\n"
        "3. DEEP_RESEARCH: Writes full markdown reports. Use for broad topics ('History of X', 'Analysis of Y'). This is slow, try to use sparingly as it takes time.\n"
        "4. MATH_LAB: Formal proofs. Use for 'prove that', 'theorem', 'lean4', or things that need to be proven without a doubt.\n"
        "5. CODE_LAB: Simulations/Scripts. Use for 'plot', 'calculate', 'simulate', or finding answers to numerical questions of any kind.\n\n"
        "Protocol:\n"
        "- Analyze the User Query and previous steps.\n"
        "- Decide the NEXT single step.\n"
        "- If you have enough info, use tool 'FINISH'.\n"
        "- Output ONLY valid JSON: {\"tool\": \"TOOL_NAME\", \"query\": \"specific instruction\"}"
    )

    # Provide history/context to the planner
    history_text = "\n".join([f"Step {i+1}: {step}" for i, step in enumerate(state["plan"])])

    user_content = (
        f"User Query: {query}\n"
        f"Local Context (Librarian): {context[:1000]}...\n"
        f"History:\n{history_text}\n\n"
        f"Current Depth: {depth}/{state['max_depth']}\n"
        "What is the next step?"
    )

    try:
        response = run_llm(ORCHESTRATOR_MODEL, user_content, system_prompt, temperature=0.1)

        # Clean response to ensure json
        content = response.strip()
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]

        return {"current_step": content.strip()}

    except Exception as e:
        # Fallback
        return {"current_step": '{"tool": "FINISH", "query": "Error in planning."}'}

def executor_node(state: OrchestratorState):
    """The Hands: Parses the JSON plan and calls the wrapper functions."""
    command_json = state["current_step"]
    plan = state["plan"]

    try:
        command = json.loads(command_json)
        tool = command.get("tool")
        query = command.get("query")
    except:
        return {"final_answer": f"Orchestrator Error: Could not parse plan: {command_json}"}

    tool_output = ""

    if tool == "LIBRARIAN":
        # We assume librarian is globally available via `librarian_instance`
        # injected during main execution
        console.print(f"\n[Orchestrator] ðŸ” Reading files for: '{query}'")
        tool_output = librarian_instance.query(query)

    elif tool == "WEB_SCOUT":
        tool_output = call_web_scout(query)

    elif tool == "DEEP_RESEARCH":
        tool_output = call_deep_research(query)

    elif tool == "MATH_LAB":
        tool_output = call_math_lab(query)

    elif tool == "CODE_LAB":
        tool_output = call_code_lab(query)

    elif tool == "FINISH":
        return {"final_answer": query} # The query here is actually the summary

    else:
        tool_output = f"Unknown Tool: {tool}"

    # Record the result
    step_record = f"TOOL: {tool}\nQUERY: {query}\nRESULT: {tool_output[:500]}..." # Truncate for history

    # Update state: append to plan, increment depth, loop back
    return {
        "plan": [step_record],
        "depth": state["depth"] + 1,
        "rag_context": state["rag_context"] + f"\n[New Info from {tool}]: {tool_output[:2000]}"
    }

# --- GRAPH CONSTRUCTION ---

def router(state: OrchestratorState):
    if "FINISH" in state["current_step"]:
        return END
    return "executor"

workflow = StateGraph(OrchestratorState)

workflow.add_node("planner", planner_node)
workflow.add_node("executor", executor_node)

workflow.set_entry_point("planner")

workflow.add_conditional_edges("planner", router, {"executor": "executor", END: END})
workflow.add_edge("executor", "planner")

orchestrator_app = workflow.compile()

def main():
    # Make librarian_instance available to executor_node via global or argument injection
    # For now, we use global as in the original design, but better would be to pass it in state or config
    global librarian_instance

    console.print(Panel("[bold white]META-ORCHESTRATOR[/bold white]\n[dim]Commanding: Math, Code, Web, Research, RAG[/dim]", style="red"))

    # 1. Initialize RAG
    path = pt_prompt("Project Path for RAG (default '.'): ", style=PtStyle.from_dict({'prompt': 'ansired bold'})) or "."
    librarian_instance = LocalLibrarian(path)
    librarian_instance.ingest()

    # 2. Get User Query
    user_input = pt_prompt("\nResearch Request > ", style=PtStyle.from_dict({'prompt': 'ansicyan bold'}))

    # 3. Initial State
    initial_state = {
        "user_query": user_input,
        "rag_context": "",
        "plan": [],
        "current_step": "",
        "final_answer": "",
        "depth": 0,
        "max_depth": MAX_DEPTH
    }

    # 4. Run
    final_state = orchestrator_app.invoke(initial_state)

    console.print("\n\n")
    console.print(Panel(Markdown(final_state["final_answer"]), title="Final Response", border_style="green"))

# --- MAIN EXECUTION ---
if __name__ == "__main__":
    main()
